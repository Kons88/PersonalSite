<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Konstantinos Kirillov</title>
  <link rel="stylesheet" href="styles.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <!-- Add Font Awesome -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
</head>
<body>
  <header>
    <!-- Background Video -->
    <div id="head" class="header-background">
      <video autoplay muted loop playsinline>
        <source src="slowdarkwaves.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>

    <!-- Header Content (Name and Navigation) -->
    <div class="header-content">
      <!-- Top Left: Name -->
      <div class="top-left">
        <h2><a href="#head">Konstantinos Kirillov</a></h2>
      </div>

      <!-- Top Right: Navigation -->
      <nav class="top-right">
        <ul>
          <li><a href="cv.html">CV</a></li>
          <li><a href="contact.html">Contact</a></li>
        </ul>
      </nav>
    </div>

    <!-- Minimalistic Message (Centered in the video) -->
    <div class="video-message">
      <h1>Data, Code and Everything in Between</h1>
      <!-- Scroll Button -->
      <a href="#projects" class="scroll-button">VIEW MY WORK</a>
    </div>
  </header>

  <!-- Main Content -->
  <main>
    <section id="projects" class="projects">
      <div class="container">
        <h2>Projects</h2>
        <div class="project-grid">
          <!-- Project 1 -->
          <div class="project-card">
            <div class="project-icon">
              <div class="project-flair">Bayes • R • WinBUGS</div>
              <i class="fas fa-wine-glass-alt"></i>
            </div>
            <h3>Choosing the Best Physiochemical Properties in a Strain of Red Wine</h3>
            <p>A study applying Variable Selection on Multinomial Logistic Regression through Bayesian Adaptive Sampling.</p>
            <a href="#" class="project-link" data-modal="modal1">View Details</a>
          </div>

          <!-- Project 2 -->
          <div class="project-card">
            <div class="project-icon">
              <div class="project-flair">Web-Scraping • R</div>
              <i class="fab fa-linkedin"></i>
            </div>
            <h3>Web-Scraping for LinkedIn</h3>
            <p>A web-scraping pipeline in R, which aims to systematically extract, process, and analyze keyword frequencies from LinkedIn job postings, enabling the identification of emerging trends and the most sought-after statistical software in the job market.</p>
            <a href="#" class="project-link" data-modal="modal2">View Details</a>
          </div>

          <!-- Project 3 -->
          <div class="project-card">
            <div class="project-icon">
              <div class="project-flair">NLP • ML • Python</div>
              <i class="fab fa-twitter"></i>
            </div>
            <h3>Classifying Disaster Tweets</h3>
            <p>An application, which combines Natural Language Processing (NLP) and Machine Learning (ML), in order to determine whether a tweet refers to a real disaster or not.</p>
            <a href="#" class="project-link" data-modal="modal3">View Details</a>
          </div>

          <!-- Project 4 -->
          <div class="project-card">
            <div class="project-icon">
              <div class="project-flair">Finance • Python</div>
              <i class='fas fa-money-bill'></i>
            </div>
            <h3>Comparative Analysis of Performance Ratios for Optimal Portfolio Selection</h3>
            <p>Optimal portfolio selection is explored through various strategies, with a comparative analysis of their performance ratios.</p>
            <a href="#" class="project-link" data-modal="modal4">View Details</a>
          </div>

          <!-- Project 5 -->
          <div class="project-card">
            <div class="project-icon">
              <div class="project-flair">R</div>
              <i class="fas fa-code"></i>
            </div>
            <h3>Binary Distances Through Similarity-Dissimilarity Measures</h3>
            <p>An algorithm in R that calculates binary distances between lines or columns of a matrix.</p>
            <a href="#" class="project-link" data-modal="modal5">View Details</a>
          </div>

          <!-- Project 6 -->
          <div class="project-card">
            <div class="project-icon">
              <div class="project-flair">GLM</div>
              <i class='fas fa-book-open'></i>
            </div>
            <h3>An Introduction to Generalized Linear Models - Solutions</h3>
            <p>Solutions to the book by Annette J. Dobson and Adrian G. Barnett.</p>
            <a href="https://kons88.github.io/IGLMSolutions/" class="link">Go to Site</a>
          </div>

        </div>
      </div>
    </section>
  </main>

  <!-- Modals for Project Details -->
  <div id="modal1" class="modal">
    <div class="modal-content">
      <span class="close">&times;</span>
      <h2 class="modal-title">Choosing the Best Physiochemical Properties in a Strain of Red Wine</h2>
      <div class="modal-body">
        <br />
        <hr />
        <h2>Abstract</h2>
        <p>This paper explores the application of Bayesian statistical methods in identifying the most influential physiochemical properties of red "Vinho Verde" wine. Using a dataset comprising 1599 observations and 11 physiochemical characteristics, we employ a multinomial logistic regression model with Bayesian variable selection techniques to determine the optimal combination of properties that contribute to higher wine quality scores. The results reveal that alcohol content, sulphates, and free sulfur dioxide have a significant positive impact on wine quality, whereas volatile acidity, chlorides, and total sulfur dioxide are negatively correlated with quality.</p>
        <hr />
        <hr />
        <br />

        <h2>1. Introduction</h2>
        <p>The wine industry has long been driven by subjective evaluations of quality, yet statistical methods can provide objective insights into the factors that influence taste and consumer preference. "Vinho Verde," a traditional wine from northwestern Portugal, is produced in various red and white strains, with some being more commercially successful than others. This study aims to determine the key physiochemical properties that contribute to higher-rated red "Vinho Verde" wines and use this knowledge to optimize wine production.</p>
        <br>

        <h2>2. Data and Methods</h2>

        <h3>2.1 Data Description</h3>
        <p>The dataset used in this analysis consists of 1599 samples of red "Vinho Verde" wine. The first 11 variables represent objective physiochemical properties, including acidity, residual sugar, sulfur dioxide content, and alcohol percentage. The 12th variable is a sensory rating provided by wine experts, ranging from 3 to 8.</p>

        <h3>2.2 Bayesian Multinomial Logistic Regression</h3>
        <p>Given the categorical nature of the response variable, we employ a multinomial logistic regression model. Bayesian inference is implemented to account for uncertainty in parameter estimation. We use low-information priors on regression coefficients and apply variable selection methods such as the Bayesian Information Criterion (BIC), empirical Bayes, and hyper-g priors.</p>
        <br>

        <h3>2.3 Model Specification</h3>
        <p>The model assumes that the probability of a wine receiving a particular score follows a multinomial (categorical) distribution:</p>
        $$ y_i \sim Multinomal(p_{i,x}) $$
        <p>for \(i=1599\), our number of observations and \(x=1:6\), the different categories scores (3-8). In this model, our reference category will be 6, which was altered in R to be the most frequent category (the one that had the most observations). Thus, for observation \(i\), the probability \(p_{ik}\)​ of belonging to category \(k\) (vs. reference \(K=6\)) is:
        $$ ln(\frac{p_{ik}}{p_{iK}}) = \beta_{k0} + \sum_{j=1}^{11}\beta_{kj} \cdot Z_{ij}, \qquad k=1,...,5, $$</p>

        <p>where \(Z_{ij}\) are standardized predictors. BAS iteratively samples models from the posterior \(p(\gamma∣y)\), prioritizing those with high marginal likelihoods.</p>
        <p><u><i>Marginal Likelihood Computation:</i></u></p>
        <p>For a model \(\gamma\), the marginal likelihood integrates out \(\beta\):</p>
        $$ p(y|\gamma) = \int p(y|\beta, \gamma) \cdot p(\beta|gamma)d\beta  $$
        <p>Under the g-prior, this is analytically tractable:</p>
        $$ p(y|\gamma) \varpropto (1+g)^{\frac{p_\gamma}{2}}[1+\frac{g}{1+g}\cdot SSR_\gamma]^{-\frac{n}{2}} $$
        <p>where \(SSR_\gamma\)​ is the sum of squared residuals for model \(\gamma\).</p>
        <br>


        <h3>2.3.1 Coefficient-Level Priors</h3>
        <p><u><i>a. Low-Information Priors (Baseline Model):</i></u><p>
        <p>For initial exploration, weakly informative Gaussian priors were assigned:</p>
        $$ \beta_{kj} \sim \mathcal{N}(0, \sigma^2 = 10^4) $$
        <p>where \(\sigma^2 = 10^4\) reflects minimal prior knowledge, allowing the likelihood to dominate posterior inference. These priors are mathematically convenient but risk overfitting in high-dimensional settings.</p>

        <p><u><i>b. Zellner’s g-Prior:</i></u><p>
        <p>For structured shrinkage, Zellner’s g-prior was applied:</p>
        $$ \beta_{kj}|g \sim \mathcal{N}(0, g \cdot (X^TX)^{-1} \cdot \sigma^2), $$
        <p>where \(g>0\) controls shrinkage strength. This prior scales coefficients by the Fisher information matrix \((X^TX)^{-1}\).</p>

        <p><u><i>c. Hyper-g Prior (Liang et al., 2008):</i></u><p>
        <p>To address \(g\) sensitivity, the hyper-g prior treats \(g\) as random with a Beta prime hyperprior:</p>
        $$ \frac{g}{1+g} \sim Beta(1, \frac{a}{2}-1), \qquad where \; a=3 \; (default). $$
        <p>Rewriting \(g=\frac{w}{1-w}\)​ with \(w \sim Beta(1,b)\), this induces a heavy-tailed prior on \(\beta_{jk}\)​:</p>
        $$ \beta_{jk} \sim \mathcal{N}(0, \frac{w}{1-w} \cdot (X^TX)^{-1} \cdot \sigma^2). $$

        <p><u><i>d. BIC Approximation:</i></u><p>
        <p>The BIC-approximated prior imposes a "unit information" penalty:</p>
        $$ p(\beta_{jk}) \varpropto exp(-\frac{1}{2}BIC), \qquad where \; BIC = -2 \cdot ln\mathcal{L}+p \cdot ln(n), $$
        <p>effectively approximating a Gaussian prior with variance \(n^{-1}\).</p>
        <br>

        <h3>2.3.2 Model-Space Priors</h3>
        <p><u><i>a. Uniform Prior:</i></u><p>
        <p>Here the main assumption is that all models \(\gamma\) (where \(\gamma_j=1\) if predictor \(j\) is included) are equally likely:</p>
        $$ p(\gamma) = \frac{1}{2^m} \qquad (for \; m=11 \; predictions) $$

        <p><u><i>b. Beta-Binomial Prior:</i></u><p>
        <p>A beta-binomial prior introduces sparsity by penalizing model size:</p>
        $$ \pi \sim Beta(1,b), \gamma_i \sim Bernoulli(\pi) $$
        <p>where \(\beta = \frac{a}{2}-1=0.5\) (with \(a=3\)). Marginalizing over \(\pi\), the prior probability of a model with \(p_\gamma\)​ predictors is:</p>
        $$ p(\gamma) \varpropto \frac{\Gamma(1+p_\gamma)\Gamma(0.5+m-p_\gamma)}{\Gamma(1.5+m)}, \qquad (for \; m=11 \; predictions) $$
        <br>

        <h3>2.4 Model Comparison via DIC</h3>
        <p>The Deviance Information Criterion (DIC) is defined as:</p>
        $$ DIC=\underbrace{−2 \cdot ln⁡p(y∣\hat{\beta})}_{\text{Deviance}} + \underbrace{2p_D}_{Penalty} $$
        <p>where \(p_D=E[−2 \cdot ln⁡p(y∣\beta)]−(−2 \cdot ln⁡p(y∣\hat{\beta}))\) estimates effective parameters. The results of prior comparisons were:</p>
        <br />
        <table class="styled-table" style = "margin:auto">
            <thead>
                <tr>
                    <th>Model</th>
                    <th>Key Behaviour</th>
                    <th>DIC Value</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Original Model (Low-Info)</td>
                    <td>Overfit, high \(p_D\)​ due to unpenalized complexity</td>
                    <td style = "text-align: right;">3036.0</td>
                </tr>
                <tr>
                    <td>Reduced Model (BIC with Beta-Binomials)</td>
                    <td>Excessively sparse, poor fit</td>
                    <td style = "text-align: right;">3063.0</td>
                </tr>
                <tr>
                    <td>Reduced Model (g-prior with Beta-Binomials)</td>
                    <td>Rigid shrinkage, misses weak signals</td>
                    <td style = "text-align: right;">3058.0</td>
                </tr>
                <tr class="active-row">
                    <td>Reduced Model (Hyper-g with Beta-Binomials)</td>
                    <td>Adaptive shrinkage, optimal fit-parsimony balance</td>
                    <td style = "text-align: right;">3017.0</td>
                </tr>
            </tbody>
        </table>
        <br />
        <p>The problem with the reduced models which used BIC prior and g-prior on the betas is that they were too strict with the selection of the statistically significant coefficients up to the point where their final models did not represent the data as well as they should and thus, had much higher DIC values than the original model.</p>
        <br />
        <h2>3. Discussing the Results</h2>
        <p>The selected variables and their exponentiated coefficients suggest the following key findings:</p>
        <ul>
            <li>Alcohol content significantly increases the probability of higher quality ratings.</li>
            <li>Sulphates and free sulfur dioxide contribute positively to wine quality when maintained at optimal levels.</li>
            <li>High levels of volatile acidity and chlorides are detrimental to wine scores.</li>
            <li>pH and density should be carefully managed to balance the sensory profile of the wine.</li>
        </ul>
        <br />
        <h2>4. Conclusion</h2>
        <p>Our Bayesian approach identifies critical physiochemical characteristics that influence the quality of red "Vinho Verde" wines. These insights offer practical implications for winemakers seeking to refine production processes. Future research may incorporate additional sensory attributes and advanced machine learning techniques to further enhance predictive accuracy.</p>
        <br />
        <h2>References</h2>
        <p>Cortez, P., Cerdeira, A., Almeida, F., Matos, T., & Reis, J. (2009). Modeling wine preferences by data mining from physicochemical properties. <em>Decision Support Systems</em>, 47(4), 547-553. https://doi.org/10.1016/j.dss.2009.05.016</p>
        <p>Liang, F., et al. (2008). <em>Mixtures of g-priors for Bayesian variable selection</em>. JASA.</p>
        <p>Zellner, A. (1986). <em>On assessing prior distributions and Bayesian regression analysis with g-prior distributions</em>. Bayesian Inference and Decision Techniques.</p>
        <p>Ntzoufras, I. 2009. <em>Bayesian Modelling Using WinBUGS</em>. Wiley.</p>
        <p>Agresti, A. (2019). <em>An introduction to categorical data analysis</em> (3rd ed.). Wiley-Interscience.</p>
        <p>Korner-Nievergelt, F., Roth, T., Felten, S., Guélat, J., Almasi, B., & Korner-Nievergelt, P. (2015). <em>Bayesian data analysis in ecology using linear models with R, BUGS, and Stan</em>. Academic Press.</p>
        <p>Best, N., Spiegelhalter, D., Thomas, A., & Lunn, D. (2012). <em>The BUGS book: A practical introduction to Bayesian analysis</em>. CRC Press.</p>
      </div>
      <div class="modal-footer">
        <a href="wine_project_code.zip" download class="download-button">Download Code</a>
      </div>
    </div>
  </div>

  <div id="modal2" class="modal">
    <div class="modal-content">
      <span class="close">&times;</span>
      <h2 class="modal-title">Web-Scraping for LinkedIn</h2>
      <div class="modal-body">
        <br />
        <hr />
        <h2>Abstract</h2>
        <p>Web scraping is an automated technique for extracting structured data from web pages. This methodology leverages the hierarchical nature of web documents, which are typically composed of HTML, CSS, and JavaScript. The objective of this study is to explore efficient web scraping techniques for extracting employment related data from LinkedIn.</p>
        <hr />
        <hr />
        <br />

        <h2>1. Introduction</h2>
        <p>Web scraping is a technique, which automatically gathers data from the internet, by using the hierarchical structure of a webpage and then exports the results into a format that is more useful to the user. However, since the webpages come in all shapes and sizes, so do the algorithms that perform web-scraping. In general, a webpage is comprised of different languages, such as HTML, CSS, JavaScript, PHP, etc. that are interlinked within its source code and have it respond dynamically to whatever a user does (like click, or point, or hover over an object with a mouse, or perhaps type a string of characters using a keyboard, etc.). Therefore, the goal of web scraping, is to utilize this unique structure and get the data that a user desires.</p>

        <br />

        <h2>2. Data Extraction from LinkedIn</h2>
        <p>Due to its dynamic content loading, LinkedIn requires advanced scraping techniques. The primary challenges include handling asynchronous JavaScript updates and mitigating access restrictions.</p>
        <br />

        <h2>3. Procedural Steps</h2>
        <ol>
            <li>Deployment of Docker to host RSelenium.</li>
            <li>Generation of target URLs based on job search parameters.</li>
            <li>Establishment of a server-client connection to navigate LinkedIn.</li>
            <li>Programmatic scrolling to render dynamic content.</li>
            <li>HTML parsing and extraction of job descriptions.</li>
            <li>Keyword analysis to classify job listings based on relevant skills.</li>
            <li>Storage of structured data in a database or structured file format.</li>
        </ol>
        <br />

        <h2>4. Results</h2>
        </br>
        <img src="Word Cloud.png" alt="Results Graph" style="
            height: 200;
            width: 200;
            border: 3px solid #ff6f61;
            border-radius: 10px;
            margin: 10px auto;
            display: block;">
        </br>
            <p><u><i>Some words about the graph above:</i></u></p>
            <p>As we can gather from the word-cloud above, the most popular data science and statistics software in 2025 is still Python, followed closely by SQL and R as far as the firms are concerned. It should come as no surprise as these are the tools most frequently mentioned both in businesses and in academia due to their simplicity in use, their ability to use powerful packages and versatility in presenting and visualizing the data. Plus, they require no subscription or fee, which is yet another incentive for any skeptical company or school. Tableau and PowerBI come right next to them, and these are proprietary software that were built with a main goal to help workers visualize and summarize the data given in a fast and easy way. These arguably have a more intuitive interface than the previous three mentioned and they do not need any programming knowledge to use them. They both have a free (or public) version of their respective software, but are, of course, more limited to what they can do when compared to the full versions. Perhaps the companies do not endorse them as much as others due to their monthly subscription.</p>

      </div>
      <div class="modal-footer">
        <a href="web_scraping.zip" download class="download-button">Download Code</a>
      </div>
    </div>
  </div>

  <div id="modal3" class="modal">
    <div class="modal-content">
      <span class="close">&times;</span>
      <h2 class="modal-title">Predicting Disaster Tweets</h2>
      <div class="modal-body">
        <br />
        <hr />
        <h2>Abstract</h2>
        <p>Natural Language Processing (NLP) leverages statistical methodologies to enable machines to understand, generate, and manipulate human language. The study pairs an NLP approach called TF-IDF with Random Forest Classification, and afterwards, evaluates their efficacy in tweet classification.</p>
        <hr />
        <hr />
        <br>

        <h2>1. Introduction</h2>
        <p>Twitter is a social media platform where users share real-time updates, ideas, and interactions through short messages called tweets. These tweets, limited to 280 characters, can include hashtags, media, and direct engagement features like replies and retweets, enabling global conversations on diverse topics. The disaster tweets dataset on Kaggle focuses on leveraging this real-time data for natural language processing (NLP) and machine learning (ML) tasks. Its goal is to classify tweets as either referencing real disasters (e.g., earthquakes, fires) or non-disaster content, aiming to build accurate models that assist crisis response teams in identifying emergencies swiftly. By distinguishing actionable disaster-related tweets from casual mentions, this dataset supports improved situational awareness and disaster management efforts.</p>
        <br>

        <h2>2. Term Frequency-Inverse Document Frequency (TF-IDF)</h2>
        <p>Term Frequency-Inverse Document Frequency (TF-IDF) is a numerical statistic that evaluates the importance of a word in a document relative to a collection (corpus). It helps distinguish words that are informative for a document while reducing the weight of common words that appear frequently across all documents.</p>
        <br />
        <p><i><u>a. Term Frequency (TF):</u></i></p>
        <p>Term Frequency (TF) measures how often a term appears in a document. It is calculated as:</p>
        $$ TF(t,d) = \frac{f_{t,d}}{N_d} $$
        <p>where:</p>
        <ul>
            <li>\(f_{t,d}\) is the raw count of term \(t\) in document \(d\),</li>
            <li>\(N_d\) is the total number of terms in document \(d\).</li>
        </ul>
        <p>This normalization accounts for document length, ensuring that longer documents don’t automatically have higher term counts.</p>
        <br>
        <p><i><u>b. Inverse Document Frequency (IDF):</u></i></p>
        <p>Inverse Document Frequency (IDF) measures the importance of a term across a collection of documents. It is defined as:</p>
        $$ IDF(t) = log(\frac{N}{df_t}+1) $$
        <p>where:</p>
        <ul>
            <li>\(N\) is the total number of documents in the corpus,</li>
            <li>\(df_t\) is the number of documents containing term \(t\).</li>
        </ul>
        <p>The logarithm dampens the impact of very frequent words while ensuring rare words have a higher weight.</p>
        <br>
        <p><i><u>c. Term Frequency-Inverse Document Frequency (TF-IDF) Calculation:</u></i></p>
        <p>TF-IDF is obtained by multiplying the term frequency and inverse document frequency:</p>
        $$ \text{TF-IDF}(t,d)=TF(t,d)×IDF(t) $$
        <p>This weighting scheme increases the importance of words that are frequent in a specific document but rare across the entire corpus. Words that appear in nearly all documents (e.g., "the," "is") have an IDF close to zero, reducing their impact.</p>
        <br>

        <h2>3. Random Forest Classification</h2>
        <p>Random Forest is an ensemble learning method that combines multiple decision trees to improve classification accuracy and reduce overfitting. It operates by constructing multiple decision trees during training and outputting the class that is the mode of individual trees’ predictions.</p>
        <br>
        <p><i><u>Decision Trees as Base Learners:</u></i></p>
        <p>Each tree in a Random Forest is a Decision Tree, which recursively splits the dataset based on feature values to minimize a chosen impurity measure, such as Gini Impurity or Entropy.</p>
        <p><i><u>a. Gini Impurity</u></i></p>
        <p>The Gini impurity measures how often a randomly chosen element from a set would be incorrectly classified if randomly labeled according to class distributions. This was the impurity measure chosen for our results, simply becaue it produced better results than an Entropy impurity measure. Its formula is:</p>
        $$ G = 1 - \sum_{i=1}^{C}p_i^2 $$
        <p>where:</p>
        <ul>
            <li>\(C\) is the total number of classes,</li>
            <li>\(p_i\) is the proportion of instances belonging to class \(i\) at a node.</li>
        </ul>
        <p>A lower Gini impurity indicates a purer node.</p>
        <p><i><u>b. Entropy (Information Gain)</u></i></p>
        <p>Entropy quantifies the disorder (uncertainty) in the dataset:</p>
        $$ H(S) = -\sum_{i=1}^{C}p_i \cdot log_2(p_i) $$
        <p>where:</p>
        <ul>
           <li>\(S\) is the set of instances at a node,</li>
           <li>\(p_i\) is the probability of class \(i\) at that node.</li>
        </ul>
        <p>A split that maximizes Information Gain (IG) is preferred:</p>
        $$ IG = H(S)-\sum_{j}\frac{|S_j|}{|S|} \cdot H(S_j) $$
        <p>where \(S_j\)​ are the subsets formed after a split.</p>
        <p><i><u>Bootstrap Aggregation (Bagging):</u></i></p>
        <p>Each tree is trained on a different bootstrap sample (random sampling with replacement) from the training data. Given \(N\) training samples, each tree is trained on approximately 63.2% of the data (since the probability of a sample not being selected is \((1−\frac{1}{N})^N \approx e^{−1} \approx 36.8%\)).</p>
        <p>The advantage of bagging is that it reduces variance by averaging multiple weak learners.</p>
        <p><i><u>Feature Randomness (Feature Selection per Split):</u></i></p>
        <p>Instead of using all features at each split (as in traditional decision trees), Random Forest selects a random subset of mm features from the total \(M\) features (\(m < M\)). This ensures diversity among trees and reduces correlation between them.</p>
        <p>The optimal \(m\) is often set as \(m = \sqrt{M}\) for classification problems, or \(m = M/3\) for regression problems.</p>
        <p><i><u>Final Prediction (Voting Mechanism):</u></i></p>
        <p>For classification tasks, each tree provides a class prediction, and the final output is determined by <strong>majority voting</strong>:</p>
        $$ \hat{y} = \arg \max_{k} \sum_{j=1}^{T}1(h_j(x)=k) $$
        <p>where:</p>
        <ul>
            <li>\(h_j(x)\) is the class prediction of the \(j^{th}\) tree for instance \(x\),</li>
            <li>\(T\) is the total number of trees,</li>
            <li>\(1(\cdot)\) is an indicator function that returns \(1\) if the condition is true.</li>
        </ul>
        <p>For regression tasks, the final output is the average of all tree predictions:</p>
        $$ \hat{y} = \frac{1}{T} \sum_{j=1}^{T}h_j(x) $$
        <p><i><u>Bias-Variance Tradeoff in Random Forest:</u></i></p>
        <p>Random Forest reduces variance compared to individual decision trees while maintaining low bias. This is because bagging reduces variance by averaging out noise and random feature selection prevents trees from being too similar. However, using too many trees increases computational cost, while too few trees can lead to underfitting.</p>
        <br>

        <h2>4. F1 Score: A Statistical Measure of Classification Performance</h2>
        <p>The F1 score is a harmonic mean of precision and recall, widely used to evaluate binary classification models, particularly in imbalanced datasets where one class dominates. It balances the trade-off between precision (accuracy of positive predictions) and recall (completeness of positive identifications).</p>
        <p><i><u>Precision (P):</u></i></p>
        <p>Precision measures how many of the predicted positive instances are actually correct:</p>
        $$ P = \frac{TP}{TP+FP} $$
        <p>where:</p>
        <ul>
           <li>True Positives (\(TP\)) are the correctly predicted positive samples,</li>
           <li>False Positives (\(FP\)) are the incorrectly predicted positive samples.</li>
        </ul>
        <p>Thus, it is reasonable that fewer False Positives (\(FP\)) lead to a higher Precision (\(P\)).</p>
        <p><i><u>Recall (R):</u></i></p>
        <p>Recall (also known as Sensitivity or True Positive Rate) measures how many actual positive instances were correctly identified:</p>
        $$ R = \frac{TP}{TP+FN} $$
        <p>where False Negatives (\(FN\)) are the positive samples, which were incorrectly predicted as negative. Therefore, fewer False Negatives (\(FN\)) lead to higher Recall (\(R\)).</p>
        <p><i><u>F1-Score:</u></i></p>
        <p>The F1 score provides a single metric to evaluate classifiers in scenarios where both false positives and false negatives carry consequences. Its harmonic mean formulation ensures neither precision nor recall dominates, making it indispensable in imbalanced or high-stakes domains like healthcare, fraud detection, and NLP. It is the harmonic mean of precision and recall:</p>
        $$ F1 = 2 × \frac{P×R}{P+R} $$
        <p>The harmonic mean ensures that both precision and recall contribute equally to the score. Unlike the arithmetic mean, the harmonic mean penalizes extreme values, meaning a low precision or recall will significantly reduce the F1-Score. F1 is ideal when class distribution is skewed. For instance, in fraud detection (99% non-fraudulent transactions), accuracy alone may mislead (e.g., 99% accuracy by always predicting "non-fraud"). F1 prioritizes balancing precision and recall.</p>
        <p><i><u>Interpretation and Properties:</u></i></p>
        <ul>
           <li>Best Case: \(F1=1\) when \(P=R=1\) (perfect classification).</li>
           <li>Worst Case: \(F1=0\) when either \(P=0\) or \(R=0\) (completely incorrect classification).</li>
           <li>Balanced Contribution: If precision is high but recall is low (or vice versa), the F1-score provides a balanced view rather than favoring one over the other.</li>
        </ul>
        <br>

        <h2>5. Results</h2>
        <p>From the very beginning it was apparent that a clear distinction between disaster-related tweets and ordinary social media posts could probably be achieved. This is something that is corroborated by the word-clouds below:</p>
        <div style="display: flex; gap: 20px; margin: 0; padding: 0;">
            <div style="flex: 1; margin: 0; padding: 0;">
                <img src="disaster1.png" alt="Disasters Graph"
                     style="width: 200; height: 200; display: block; border: 3px solid #ff6f61; border-radius: 10px; margin: 10px auto;">
            </div>
            <div style="flex: 1; margin: 0; padding: 0;">
                <img src="disaster2.png" alt="Non-Disasters Graph"
                     style="width: 200; height: 200; display: block; border: 3px solid #ff6f61; border-radius: 10px; margin: 10px auto;">
            </div>
        </div>
        <p>It appears that the most frequent words get re-used in both sets of data. Many people must be linking websites or videos to their tweets (and thus, "https" is the top result in the word-clouds) sharing either their opinions, or real disasters. The word cloud that depicts disasters, seems to include names for those disasters, like FIRE, or STORM, etc. used more frequently. And also on a first glance, it uses more "negative" words, like KILLED, or CRASH as well as others.</p>
        <p>The evaluation of our results yielded the following:</p>
        <table class="styled-table" style = "margin:auto">
            <thead>
                <tr>
                    <th></th>
                    <th style = "text-align: center;">Precision</th>
                    <th style = "text-align: center;">Recall</th>
                    <th style = "text-align: center;">F1-Score</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>0</td>
                    <td style = "text-align: center;">0.76</td>
                    <td style = "text-align: center;">0.90</td>
                    <td style = "text-align: center;">0.82</td>
                </tr>
                <tr>
                  <td>1</td>
                  <td style = "text-align: center;">0.82</td>
                  <td style = "text-align: center;">0.61</td>
                  <td style = "text-align: center;">0.70</td>
                </tr>
            </tbody>
        </table>
        <br>
        <p>The model achieved a validation accuracy of 77.68%, indicating solid overall performance in classifying disaster tweets. However, a deeper look at precision, recall, and F1-Score provides a more nuanced understanding.</p>
        <p>In general, the model performs better at identifying non-disaster tweets (Class 0), with a high recall (0.90), meaning it correctly captures most of them. However, for disaster tweets (Class 1), recall is lower (0.61), meaning many real disaster tweets are misclassified as non-disasters. This is a concern in critical applications where false negatives (missed disasters) are costly.</p>
        <br>

        <h2>6. Conclusion</h2>
        <p>This study applied Natural Language Processing (NLP) techniques, specifically TF-IDF for feature extraction and a Random Forest classifier, to classify disaster-related tweets. The model achieved a validation accuracy of 77.68%, with strong precision for disaster tweets (0.82) but lower recall (0.61), indicating that while false positives were minimized, some disaster-related tweets were misclassified.</p>
        <p>Improving recall through class balancing, threshold adjustments, or enhanced feature engineering could further refine performance. Overall, this approach demonstrates the effectiveness of statistical NLP and machine learning in disaster tweet classification, with potential for real-time application in crisis response.</p>
        <br />

        <h2>References</h2>
        <p>Addison Howard, Devrishi, Phil Culliton, and Yufeng Guo. (2019). <em>Natural Language Processing with Disaster Tweets</em>. Retrieved from https://www.kaggle.com/competitions/nlp-getting-started</p>
        <p>Spärck Jones, K. (1972). <em>A statistical interpretation of term specificity and its application in retrieval</em>. Journal of Documentation, 28(1), 11–21. https://doi.org/10.1108/eb026526</p>
        <p>Breiman, L. (2001). <em>Random forests. Machine Learning</em>, 45(1), 5–32. https://doi.org/10.1023/A:1010933404324</p>
        <p>Hastie, T., Tibshirani, R., & Friedman, J. (2009). <em>The elements of statistical learning: Data mining, inference, and prediction</em> (2nd ed.). Springer. https://doi.org/10.1007/978-0-387-84858-7</p>
        <p>Géron, A. (2019). <em>Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow</em> (2nd ed.). O’Reilly Media.</p>
      </div>
      <div class="modal-footer">
        <a href="tweet_disaster.zip" download class="download-button">Download Code</a>
      </div>
    </div>
  </div>

  <div id="modal4" class="modal">
    <div class="modal-content">
      <span class="close">&times;</span>
      <h2 class="modal-title">Comparative Analysis of Performance Ratios for Optimal Portfolio Selection</h2>
      <div class="modal-body">
        <br />
        <hr />
        <h2>Abstract</h2>
        <p>This study conducts a comprehensive evaluation of various portfolio optimization strategies using historical stock data to identify the most effective approaches for maximizing returns while minimizing risk. We analyze six distinct strategies: <strong>Equal-Weighted</strong>, <strong>Minimum Volatility</strong>, <strong>Maximum Sharpe Ratio</strong>, <strong>Risk Parity</strong>, <strong>Momentum</strong>, and <strong>Monte Carlo Simulation-Based</strong> portfolios. Key performance metrics, including <strong>Annualized Return</strong>, <strong>Volatility</strong>, <strong>Sharpe Ratio</strong>, and <strong>Sortino Ratio</strong>, are employed to assess their effectiveness. The findings reveal that <strong>Momentum</strong> and <strong>Maximum Sharpe Ratio</strong> portfolios consistently outperform traditional <strong>Equal-Weighted</strong> strategies, particularly under specific market conditions. This research underscores the importance of advanced optimization techniques in achieving superior risk-adjusted returns.</p>
        <hr />
        <hr />
        <br>

        <h2>1. Introduction</h2>
        <p>Portfolio optimization is a cornerstone of modern investment management, aiming to construct asset allocations that efficiently balance risk and return. Traditional models, such as Markowitz's mean-variance optimization, have laid the foundation for portfolio theory. However, advancements in financial analytics have introduced more sophisticated techniques, including factor-based investing, risk parity, and Monte Carlo simulations. This paper investigates the efficacy of six portfolio strategies applied to a dataset of ten major U.S. stocks (<i>"AAPL", "MSFT", "GOOGL", "TSLA", "JNJ", "PFE", "XOM", "CVX", "WMT", "PG"</i>) from January 2020 to January 2025. By employing risk-adjusted performance metrics, we aim to identify the most effective strategies for investors seeking to optimize their portfolios.</p>
        <br>

        <h2>2. Portfolio Strategies</h2>
        <h3><strong><u> 2.1. The Equal-Weights Portfolio </u></strong>:</h3>

        <p>An Equal-Weighted Portfolio assigns the same weight to each asset in the portfolio, regardless of its market capitalization, risk, or return characteristics. Each asset in the portfolio is given an equal percentage allocation (for instance in our 10-asset portfolio, each stock gets 10%). It does require constant rebalancing to maintain equal weights as asset prices change. Mathematically, its weights could be written as:</p>
        $$ w_i = \frac{1}{n}, \qquad for \; i=1,...,n. $$
        <p>with:</p>
        $$ \sum_{i=1}^{n}w_i=1 $$

        <p>where:</p>
        <ul>
            <li>\(w_i\) is the weight of asset \(i\).</li>
            <li>\(n\) is the total number of  assets.</li>
        </ul>

        <p>This formula gets rebalanced as asset prices change over periods.</p>

        <p>Despite its simplicity, the equal-weighted approach has several advantages:</p>
        <ol>
            <li><strong>Simplicity</strong>: Due to its simplicity, it does not require complex optimization algorithms or forecasting models.</li>
            <li><strong>Diversification</strong>: It ensures diversification across all selected assets.  The portfolio itself is not overly concentrated in any single asset.</li>
            <li><strong>Reduced Bias</strong>: It avoids the biases that can creep into other weighting schemes, like market-cap weighting, which tends to overweight large companies.</li>
        </ol>

        <p><u>Important Considerations for Equal-Weighted Portfolios:</u></p>
        <ol>
            <li><strong>Rebalancing</strong>: Equal-weighted portfolios require periodic rebalancing. As asset prices change, the weights will drift away from the equal allocation. Rebalancing involves selling some of the overperforming assets and buying some of the underperforming assets to restore the equal weights. This rebalancing process can incur transaction costs, which can eat into returns. The frequency of rebalancing has to be chosen carefully.</li>
            <li><strong>Volatility</strong>: While equal weighting diversifies, it doesn't eliminate volatility.  The portfolio can still experience significant fluctuations, especially if the underlying assets are volatile. Furthermore, equal weighting can inadvertently lead to a bias towards smaller companies, as they receive the same weight as large companies. This can increase volatility.</li>
        </ol>
        <br>
        <hr>
        <br>

        <h3><strong><u> 2.2. The Minimum Volatility Portfolio </u></strong>:</h3>
        <p>A Minimum Volatility Portfolio is constructed to minimize the overall portfolio volatility (standard deviation of returns). What it does, is it seeks the portfolio weights that result in the lowest possible portfolio variance, regardless of the expected returns of the individual assets. It can be expressed mathematically as:</p>
        $$ \min_{w}w^T \Sigma w $$
        <p>subject to:</p>
        $$ \sum_{i=1}^{n}w_i=1, \qquad and \qquad w_i \geq 0 \qquad (no \; short \; selling)$$
        <p>where:</p>
        <ul>
            <li>\(w\) is the vector of asset weights \(w=[w_1, ..., w_n]\).</li>
            <li>\(\Sigma\) is the covariance matrix of asset returns.</li>
        </ul>

        <p>A Minimum Volatility Portfolio can be quite advantageous for a couple of reasons:</p>
        <ol>
            <li><strong>Risk Aversion</strong>:  Many investors are primarily concerned with minimizing risk. The MVP directly addresses this concern by focusing on minimizing volatility.</li>
            <li><strong>Diversification Benefits</strong>:  By carefully considering the covariance between assets, the MVP can achieve better diversification benefits than a naive approach (like equal weighting). It seeks to combine assets that have low or even negative correlations to reduce overall portfolio volatility.</li>
        </ol>

        <p><u>Important Considerations for Minimum Volatility Portfolios:</u></p>
        <ol>
            <li><strong>Covariance Matrix</strong>: The accuracy of the Minimum Volatility Portfolio depends heavily on the accuracy of the covariance matrix.  Estimating the covariance matrix is a crucial and often challenging task.  Errors in the covariance matrix can lead to suboptimal portfolios. Therefore it is sensitive to inputs.</li>
            <li><strong>Concentration Risk</strong>:  While the Minimum Volatility Portfolio minimizes overall volatility, it might concentrate investments in a small number of low-volatility assets. This can lead to unintended concentration risk.</li>
            <li><strong>Returns are Not Considered</strong>: The Minimum Volatility Portfolio only focuses on minimizing volatility. It does not explicitly consider expected returns.  It's possible that the Minimum Volatility Portfolio might have a low return compared to other portfolios with slightly higher volatility.</li>
        </ol>
        <br>
        <hr>
        <br>

        <h3><strong><u> 2.3. The Maximum Sharpe Ratio Portfolio </u></strong>:</h3>
        <p>A Maximum Sharpe Ratio Portfolio aims to maximize the risk-adjusted return, as measured by the Sharpe Ratio. So what it does, is it uses optimization to allocate weights to assets such that the portfolio’s Sharpe Ratio (excess return per unit of risk) is maximized. Its optimiazation formula is:</p>
        $$ \max_{w} \frac{w^TR-R_f}{\sqrt{w^T \Sigma w}} $$
        <p>subject to:</p>
        $$ \sum_{i=1}^{n}w_i=1, \qquad and \qquad w_i \geq 0$$

        <p>where:</p>
        <ul>
            <li>\(R\) is the vector of expected asset returns.</li>
            <li>\(R_f\) is the risk-free rate.</li>
            <li>\(\Sigma\) is the covariance matrix of asset returns.</li>
        </ul>

        <p>It does present a few advantages:</p>
        <ol>
            <li><strong>Optimal Risk-Return Tradeoff</strong>: The Sharpe Ratio is a widely accepted measure of risk-adjusted return.  Maximizing it means finding the portfolio that offers the best balance between risk and return.</li>
            <li><strong>Efficient Frontier</strong>: The Maximum Sharpe Ratio Portfolio is a key point on the efficient frontier. The efficient frontier represents the set of optimal portfolios that offer the highest expected return for a given level of risk (or the lowest risk for a given expected return).</li>
        </ol>

        <p><u>Important Considerations for Maximum Sharpe Ratio Portfolios:</u></p>
        <ol>
            <li><strong>Estimation Error</strong>: Because the inputs are estimates, there is always some degree of estimation error. This can lead to the "optimizer's curse," where the optimized portfolio performs worse out-of-sample than it did in-sample. ALso it is sensitive to inputs, much like the Minimum Volatility Portfolios are.</li>
            <li><strong>Normally Distributed Returns</strong>:  The Sharpe Ratio, and therefore the MSRP, assumes that returns are normally distributed.  If returns are non-normal (e.g., skewed or have fat tails), the Sharpe Ratio might not be the most appropriate metric to optimize.</li>
        </ol>
        <br>
        <hr>
        <br>

        <h3><strong><u> 2.4. The Risk Parity Portfolio </u></strong>:</h3>
        <p>A Risk Parity Portfolio aims to allocate capital such that each asset (or asset class) contributes equally to the overall portfolio risk.  Unlike traditional portfolio construction methods that focus on maximizing returns or minimizing volatility, risk parity focuses on equalizing risk contributions, so it offers a different approach to portfolio construction than market-cap weighting or other traditional methods. It can be particularly attractive for investors seeking balanced risk exposure across asset classes, particularly in volatile markets. Mathematically, it can be written as:</p>
        $$ w_i \cdot \frac{\partial \sigma_p}{\partial w_i} = w_j \cdot \frac{\partial \sigma_p}{\partial w_j}, \qquad for \; all \; i,j. $$

        <p>or alternatively, the optimization problem can be written as:</p>
        $$ \min_{w} \sum_{i=1}^{n} (w_i \cdot \frac{\partial \sigma_p}{\partial w_i} - \frac{\sigma_p}{n})^2 $$

        <p>subject to:</p>
        $$ \sum_{i=1}^{n}w_i=1, \qquad and \qquad w_i \geq 0$$

        <p>where:</p>
        <ul>
            <li>\(\sigma_p\) is the portfolio standard deviation.</li>
            <li>\(\frac{\partial \sigma_p}{\partial w_i}\) is the marginal contribution to risk (MCR) of asset \(i\). It represents how much the portfolio's overall volatility (\(\sigma_p\)) changes when you slightly change the weight of asset \(i\) (\(w_i\)), holding the weights of all other assets constant. In simpler terms, it measures how much asset \(i\) contributes to the portfolio's total risk.</li>
        </ul>

        <p>The advantages of this portfolio strategy are:</p>
        <ol>
            <li><strong>Diversification of Risk</strong>: Risk Parity Portfolios are designed to diversify risk, not just capital.  The idea is to avoid concentrating risk in a few volatile assets, which can happen with other portfolio construction methods. Thus it would perform well, where for example traditional 60/40 portfolios would struggle.</li>
            <li><strong>Stability</strong>: By equalizing risk contributions, risk parity portfolios can be more stable over time, as no single asset dominates the portfolio's risk.</li>
        </ol>

        <p><u>Important Considerations for Risk Parity Portfolios:</u></p>
        <ol>
            <li><strong>Covariance Matrix</strong>: Like other optimization methods, Risk Parity relies heavily on the accuracy of the covariance matrix.  Errors in the covariance matrix can lead to suboptimal or unintended risk allocations.</li>
            <li><strong>Rebalancing</strong>: Risk parity portfolios require frequent rebalancing to maintain the target risk contributions.  As asset volatilities and correlations change, the weights need to be adjusted. Thus, it could get expensive, when transaction costs are considered.</li>
            <li><strong>Concentration Risk</strong>: While risk parity aims to diversify risk, it might still lead to concentrated positions in certain assets, especially if a few assets have very low correlations with the rest of the portfolio.</li>
            <li><strong>Returns are Not Explicitly Considered</strong>: Risk parity focuses on equalizing risk contributions, not on maximizing returns.  The resulting portfolio might have a lower expected return than other optimized portfolios.</li>
        </ol>
        <br>
        <hr>
        <br>

        <h3><strong><u> 2.5. The Momentum Portfolio</u></strong>:</h3>

        <p>A momentum portfolio invests in assets that have performed well (shown positive price momentum) over a recent period. Essentially, what it does is, it identifies assets with the highest returns over a specific look-back period (e.g., 6-12 months) and then, it allocates higher weights to high-momentum assets and lower weights to low-momentum assets. This strategy is based on the idea that trends tend to persist – assets that have been going up are more likely to continue going up, at least for a while It is suitable for aggressive investors who can tolerate higher risk and turnover. It follows the following strategy:</p>
        <ol>
            <li>It ranks assets by their past returns over a look-back period (e.g., 6-12 months):</li>
            $$ r_i​(t) = \; Return \; of \; asset \; i \; over \; the \; look-back \; period. $$
            <li>Then, it assigns weights that are proportional to the rank of each asset:</li>
            $$ w_i = \frac{Rank(r_i)}{\sum_{j=1}^{n}Rank(r_j)} $$
        </ol>

        <p>Some of its advantages are:</p>
        <ol>
            <li><strong>Trend Following</strong>: Momentum investing is a form of trend following. It capitalizes on the tendency of asset prices to move in trends, both upward and downward.</li>
            <li><strong>Behavioral Bias</strong>: Momentum effects are sometimes attributed to behavioral biases, such as herding behavior and the tendency of investors to underreact to new information, which represents reality quite adequately.</li>
        </ol>

        <p><u>Important Considerations for Momentum Portfolios:</u></p>
        <ol>
            <li><strong>Momentum Period and Number of Top Assets</strong>: The choice of that period is crucial. Too short a period might lead to whipsaws (frequent changes in the portfolio), while too long a period might miss shorter-term trends. The number of top-performing assets to include in the portfolio is another important parameter. A smaller number might lead to a more concentrated portfolio, while a larger one might dilute the momentum effect.</li>
            <li><strong>Rebalancing Frequency</strong>: Momentum portfolios require periodic rebalancing to maintain the desired exposure to the top-performing assets. The frequency of rebalancing affects transaction costs and portfolio turnover.</li>
            <li><strong>Volatility</strong>: Momentum strategies can be volatile, especially during periods of market reversals. Which also mean s that Momentum strategies are susceptible to "crash risk."  If the market suddenly reverses, the momentum portfolio can experience large losses.</li>
        </ol>
        <br>
        <hr>
        <br>

        <h3><strong><u> 2.6. The Monte Carlo Simulation Portfolio</u></strong>:</h3>
        <p>A Monte Carlo simulation is a computational technique that uses random sampling to obtain numerical results. In the context of portfolio optimization, it involves generating a large number of random portfolios, calculating their performance metrics, and then analyzing the distribution of these results to identify potentially optimal portfolios. It uses random sampling and statistical modeling to simulate a wide range of possible portfolio outcomes. Its methodology is:</p>
        <ol>
        <li>Simulate \(M\) scenarios of asset returns using historical data or assumed distributions:</li>
        $$ R_m=[R_{1m},R_{2m},…,R_{nm}] \; for \; m=1,2,…,M $$

        <li>For each scenario, compute portfolio returns:</li>
        $$ R_{p,m} = w^TR_m $$

        <li>Optimize portfolio weights to maximize expected utility or minimize risk across all scenarios:</li>
        $$ \max_{w} \frac{1}{M} \sum_{m=1}^{M} U(R_{p,m}​) $$
        <p>Where \(U(\cdot)\) is a utility function (e.g., mean-variance utility).</p>
        </ol>

        <p>It can be pretty advantegous to use, due to its simplicity but also because of the following reasons:</p>
        <ol>
            <li><strong>Exploration of the Solution Space</strong>: Monte Carlo simulations can explore a wide range of possible portfolio weights, especially when dealing with a large number of assets. This can be helpful in identifying portfolios that might be missed by other optimization methods.</li>
            <li><strong>No Assumptions About Return Distribution</strong>: Monte Carlo simulations do not require any specific assumptions about the distribution of asset returns. This can be an advantage when dealing with assets that have non-normal return distributions.</li>
        </ol>

        <p><u>Important Considerations for Monte Carlo Simulation Portfolios:</u></p>
        <ol>
            <li><strong>Computational Cost</strong>: Monte Carlo simulations can be computationally expensive, especially when dealing with a large number of assets or a large number of simulations. More iterations generally lead to better results but also increase computation time.</li>
            <li><strong>Suboptimal Solutions</strong>: Monte Carlo simulations do not guarantee finding the absolute optimal portfolio. They provide a sample of potentially good portfolios, but there might be even better portfolios that were not generated in the simulation.</li>
            <li><strong>Random Number Generation</strong>: The quality of the random numbers used in the simulation is important.  A good random number generator is essential. It relies on the quality of input assumptions and historical data.</li>
        </ol>
        <br>
        <hr>
        <hr>

        <h2>3. Performance Metrics</h2>
        <h3><strong><u> 3.1. The Sharpe Ratio </u></strong>:</h3>

        <p>The Sharpe Ratio is one of the most widely used measures of risk-adjusted return. It was developed by Nobel laureate William F. Sharpe. The Sharpe Ratio measures the excess return (return above the risk-free rate) per unit of total risk (volatility). A higher Sharpe Ratio indicates better risk-adjusted performance. It is useful for comparing the performance of different portfolios or funds, especially when the risk-free rate is stable. It's formula is:</p>
        $$ Sharpe \; Ratio = \frac{R_p-R_f}{\sigma_p} $$

        <p>where:</p>
        <ul>
            <li>\(R_p\) is the portfolio return.</li>
            <li>\(R_f\) is the return of a theoretically risk-free investment.  In practice, it's often proxied by the yield on a short-term government bond, like a Treasury bill (which was the method used here).  The idea is that this is the return you could get with virtually no risk.  We subtract the risk-free rate because we're interested in the excess return – the return above and beyond what you could get risk-free.</li>
            <li>\(\sigma_p\) is the standard deviation of portfolio returns.</li>
        </ul>

        <p>Limitations:</p>
        <ol>
            <li>It assumes returns are normally distributed, which may not always be true.</li>
            <li>It penalizes both upside and downside volatility equally.</li>
            <li>Changes in the risk-free rate can affect the Sharpe Ratio, even if the portfolio's performance remains the same.</li>
        </ol>
        <br>
        <hr>
        <br>

        <h3><strong> <u> 3.2. The Sortino Ratio </u> </strong>:</h3>
        <p>The Sortino Ratio measures the excess return per unit of downside risk. It is more focused on the risk of losses rather than overall volatility. It is defined as:</p>
        $$ Sortino \; Ratio = \frac{E[R_p]-R_f}{\sigma_d} $$
        <p>where:</p>
        <ul>
            <li>\(E[R_p] = \sum_{i=1}^{n} w_i \cdot E[R_i] \) is the expected portfolio return. Here, \(n\) is the number of assets in a portfolio and \(w_i\) are the asset weights.</li>
            <li>\(R_f\) is the risk-free rate.</li>
            <li>\(\sigma_d\) is the downside deviation.</li>
        </ul>
        <p>Here, downside deviation \(\sigma_d\) is computed as:</p>
        $$ \sigma_d = \sqrt{\frac{1}{n} \sum_{t=1}^{n} min[R_t-R_f, 0]^2} $$

        <p>While the minimum possible value for downside deviation is technically 0, it's an unrealistic edge case.  In any practical investment scenario, the downside deviation will be greater than 0 because returns will always have some degree of variability, and there will inevitably be some returns below the target. Therefore, here instead of 0, we shall make it return a very small number: 1e-6.</p>

        <p>Limitations:</p>
        <ol>
            <li>It requires a clear definition of the target return.</li>
            <li>It ignores upside volatility, which may not be suitable for all investors.</li>
        </ol>
        <br>
        <hr>
        <br>

        <h3><strong> <u> 3.3. The Omega Ratio </u> </strong>:</h3>
        <p>The Omega Ratio is a more comprehensive measure that considers the entire distribution of returns, rather than just volatility or downside risk. It compares the probability-weighted gains above a threshold (numerator) to the probability-weighted losses below the threshold (denominator). A ratio greater than 1 indicates that the investment has more gains than losses relative to the threshold. Its formula is:</p>
        $$ Omega \; Ratio = \frac{\int_{T}^{\infty}(1-F(r))dr}{\int_{-\infty}^{T}F(r)dr} $$

        <p>where:</p>
        <ul>
            <li>\(F(r)\) is the cumulative distribution function of returns.</li>
            <li>\(T\) is the threshold return (e.g., risk-free rate or target return).</li>
        </ul>

        <p>The Omega Ratio's strength lies in its ability to handle non-normal distributions.  Unlike the Sharpe Ratio, it doesn't assume returns follow a bell curve.  This makes it more suitable for investments with skewed returns or fat tails (like hedge funds or options strategies).  It also allows you to define your own threshold (T), which can be more meaningful than just using the risk-free rate.  For example, you could set T to your portfolio's target return.</p>

        <p>Limitations:</p>
        <ol>
            <li>It is computationally complex compared to other ratios.</li>
            <li>It requires a clear definition of the threshold return \(T\). Different thresholds can lead to different Omega Ratios for the same investment.  This means careful consideration must be given to selecting an appropriate threshold.  It also means that comparing Omega Ratios across investments requires using the same threshold.</li>
            <li>While a higher Omega Ratio is generally better, the magnitude of the ratio isn't as easily interpretable as the Sharpe Ratio.  It's more about comparing ratios than looking at an absolute value.</li>
        </ol>
        <br>
        <hr>
        <br>

        <h3><strong> <u> 3.4. The Treynor Ratio </u> </strong>:</h3>
        <p>The Treynor Ratio measures the excess return per unit of systematic risk (beta), making it particularly useful for diversified portfolios. evaluates how well a portfolio compensates for market risk. A higher ratio indicates better performance relative to market risk. It is best used for well-diversified portfolios where unsystematic risk is minimal. It has the following formula:</p>
        $$ Treynor \; Ratio = \frac{R_p-R_f}{\beta_p} $$

        <p>where:</p>
        <ul>
            <li>\(R_p\) is portfolio return (same as in the Sharpe Ratio).</li>
            <li>\(R_f\) is the risk-free rate (e.g., Treasury bills).</li>
            <li>\(\beta_p\) is the beta of the portfolio, which measures the sensitivity of the portfolio's returns to changes in the market return.  A beta of 1 means the portfolio's returns tend to move in line with the market.  A beta greater than 1 means the portfolio is more volatile than the market, and a beta less than 1 means it's less volatile.</li>
        </ul>

        <p>Limitations:</p>
        <ol>
            <li>It relies on beta, which assumes a linear relationship between the portfolio and the market.</li>
            <li>It is not suitable for portfolios with significant unsystematic risk.</li>
        </ol>
        <p><u>Treynor Ratio vs. Sharpe Ratio:</u></p>

        <p>The key difference between the Treynor and Sharpe Ratios is how they measure risk. The Sharpe Ratio uses total risk (standard deviation), while the Treynor Ratio uses systematic risk (beta). As a rule of thumb:</p>
        <ul>
            <li>Use the Sharpe Ratio for evaluating the performance of a total portfolio (or a portfolio that isn't fully diversified).</li>
            <li>Use the Treynor Ratio for evaluating the performance of a well-diversified portfolio.</li>
        </ul>
        <br>
        <hr>
        <br>

        <h3><strong> <u> 3.5. The Information Ratio </u> </strong>:</h3>
        <p>The Information Ratio measures the excess return of a portfolio relative to a benchmark, adjusted for tracking error (volatility of excess returns). It assesses the consistency of outperformance relative to a benchmark. A higher ratio indicates better risk-adjusted performance compared to the benchmark. It can be defined as:</p>
        $$ Information \; Ratio = \frac{R_p-R_b}{\sigma_{p-b}} $$

        <p>where:</p>
        <ul>
            <li>\(R_p\) is portfolio return (same as in the Sharpe Ratio).</li>
            <li>\(R_b\) is the benchmark return. This could be a broad market index (like the S&P 500, which was used here) or a more specific index relevant to the portfolio's investment strategy.</li>
            <li>\(\sigma_{p-b}\) is the standard deviation of the excess return (tracking error)</li>
        </ul>

        <p><strong><em>Why Use the S&P 500 as a Benchmark?</em></strong> The S&P 500 is a widely used benchmark for investment performance, especially for US equity portfolios. It tracks the performance of 500 large-cap US companies, representing a significant portion of the overall US stock market capitalization.  It's generally considered a good proxy for the overall market's performance.</p>


        <p>Limitations:</p>
        <ol>
            <li>It is highly dependent on the choice of benchmark. A different benchmark can result in a different Information Ratio for the same portfolio.  It's crucial to select a benchmark that is appropriate for the portfolio's investment strategy.</li>
            <li>It doesn't measure the total risk of the portfolio. It only measures the risk relative to the benchmark.  A portfolio with a high IR could still be quite risky in absolute terms.</li>
            <li>it only considers excess returns relative to the benchmark.  It doesn't provide information about the portfolio's absolute return or its risk relative to other investments.</li>
        </ol>

        <p><u>Information Ratio vs. Other Ratios:</u></p>
        <ul>
            <li><strong>Sharpe Ratio</strong>: The Sharpe Ratio measures risk-adjusted return relative to the risk-free rate, while the Information Ratio measures risk-adjusted excess return relative to a benchmark.</li>
            <li><strong>Treynor Ratio</strong>: The Treynor Ratio measures risk-adjusted return relative to systematic risk (beta), while the Information Ratio measures risk-adjusted excess return relative to a benchmark and its associated tracking error.</li>
        </ul>
        <br>
        <hr>
        <br>

        <h3><strong> <u> 3.6. The Ulcer Index </u> </strong>:</h3>
        <p>The Ulcer Index is a unique risk measure that focuses specifically on the pain of experiencing drawdowns.  It quantifies the depth and duration of these drawdowns, giving a sense of the "unease" or "ulcer" that investors might feel during periods of market decline. A lower index indicates fewer and shallower drawdowns, which is preferable for risk-averse investors. Its formula is written as:</p>
        $$ Ulcer \; Index = \sqrt{\frac{\sum_{i=1}^{n}D_i^2}{n}} $$

        <p>where:</p>
        <ul>
            <li>\(D_i\) is the drawdown, or in opther words: the percentage decline from the peak. Imagine an investment reaches a high point, then declines.  The drawdown is the percentage difference between that peak and the current, lower value.  It's important to note that the Ulcer Index only considers drawdowns from previous peaks, not relative to the initial investment value.</li>
            <li>\(n\) is the number of periods.</li>
        </ul>


        <p>Limitations:</p>
        <ol>
            <li> It does not consider the frequency of drawdowns. Two investments could have the same Ulcer Index, but one might have frequent small drawdowns, while the other has infrequent but large drawdowns.  Risk-averse investors might have different preferences for these two scenarios.</li>
            <li>It ignores upside volatility which some investors might view as a missed opportunity.</li>
            <li>The drawdowns are calculated relative to previous peaks, not the initial investment value.  This means that the Ulcer Index can be influenced by short-term fluctuations even within a longer-term decline.</li>
        </ol>
        <br>
        <hr>
        <br>

        <h3><strong> <u> 3.7. The Sterling Ratio </u> </strong>:</h3>
        <p>The Sterling Ratio is a risk-adjusted performance measure that focuses on the average return relative to the average drawdown. It evaluates the return per unit of drawdown risk. It answers the question: "How much return am I getting for the drawdown risk I'm taking, on average?"  A higher Sterling Ratio is generally better, indicating that the investment has generated more return for the level of drawdown risk experienced. A higher ratio indicates better performance relative to drawdowns. Its formula is:</p>
        $$ Sterling \; Ratio = \frac{R_p}{Average \; Drawdown} $$

        <p>where:</p>
        <ul>
            <li>\(R_p\) is the total return of the portfolio over the period being considered.</li>
            <li>\(Average \; Drawdown\) is the average of the maximum drawdowns over a period. It's important to note that you first identify the maximum drawdown within each sub-period (e.g., each year, each quarter), and then calculate the average of these maximum drawdowns. Thus, comparing Sterling Ratios across investments can be tricky if the investments have different investment horizons or if the average drawdowns are calculated using different time periods.</li>
        </ul>


        <p>Limitations:</p>
        <ol>
            <li>It is sensitive to the method used to calculate average drawdowns. Different methods (e.g., using annual maximum drawdowns vs. monthly maximum drawdowns) can lead to different Sterling Ratios for the same investment.  It is important to be consistent in the method used.<li>
            <li>Like the Ulcer Index, the Sterling Ratio doesn't consider the frequency of drawdowns.  It only looks at the average of the maximum drawdowns.  An investment with frequent small drawdowns could have a similar Sterling Ratio to an investment with infrequent but large drawdowns.</li>
        </ol>

        <p><u>Sterling Ratio vs. Other Risk Measures:</u></p>
        <ul>
            <li><strong>Sharpe Ratio</strong>: The Sharpe Ratio uses standard deviation (total volatility) as the risk measure, while the Sterling Ratio uses average drawdown.</li>
            <li><strong>Ulcer Index</strong>: Both the Ulcer Index and the Sterling Ratio focus on drawdowns, but the Ulcer Index considers the duration of drawdowns, while the Sterling Ratio averages the maximum drawdowns.</li>
        </ul>
        <br>

        <h2>4. Results and Discussion</h2>
        <p>The results can be summarized in the following two tables below:</p>
        <section>
            <p><u>Portfolio Performance Analysis</u>:</p>
            <table class="styled-table">
                <thead>
                    <tr>
                        <th>Portfolio</th>
                        <th>Annual Return</th>
                        <th>Volatility</th>
                        <th>Sharpe Ratio</th>
                        <th>Sortino Ratio</th>
                        <th>Omega Ratio</th>
                        <th>Treynor Ratio</th>
                        <th>Information Ratio</th>
                        <th>Ulcer Index</th>
                        <th>Sterling Ratio</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Equal-weighted</td>
                        <td>0.2267</td>
                        <td>0.2066</td>
                        <td>0.8554</td>
                        <td>1.0425</td>
                        <td>0.0017</td>
                        <td>0.198</td>
                        <td>1.0198</td>
                        <td>0.0639</td>
                        <td>4.1592</td>
                    </tr>
                    <tr>
                        <td>Min Volatility</td>
                        <td>0.1289</td>
                        <td>0.1649</td>
                        <td>0.4781</td>
                        <td>0.6195</td>
                        <td>0.0013</td>
                        <td>0.1366</td>
                        <td>-0.0878</td>
                        <td>0.0483</td>
                        <td>2.3131</td>
                    </tr>
                    <tr>
                        <td>Max Sharpe</td>
                        <td>0.3802</td>
                        <td>0.2761</td>
                        <td>1.1961</td>
                        <td>1.6666</td>
                        <td>0.0022</td>
                        <td>0.359</td>
                        <td>1.225</td>
                        <td>0.1</td>
                        <td>4.4992</td>
                    </tr>
                    <tr>
                        <td>Risk Parity</td>
                        <td>0.2995</td>
                        <td>0.2697</td>
                        <td>0.925</td>
                        <td>1.1568</td>
                        <td>0.0026</td>
                        <td>0.2206</td>
                        <td>1.2762</td>
                        <td>0.0846</td>
                        <td>4.5093</td>
                    </tr>
                    <tr>
                        <td>Momentum</td>
                        <td>0.3249</td>
                        <td>0.252</td>
                        <td>1.091</td>
                        <td>1.4668</td>
                        <td>0.0018</td>
                        <td>0.2804</td>
                        <td>1.3054</td>
                        <td>0.1081</td>
                        <td>3.7819</td>
                    </tr>
                    <tr>
                        <td>Monte Carlo</td>
                        <td>0.372</td>
                        <td>0.2838</td>
                        <td>1.1344</td>
                        <td>1.5656</td>
                        <td>0.0026</td>
                        <td>0.3284</td>
                        <td>1.2019</td>
                        <td>0.1057</td>
                        <td>4.1396</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <section>
            <p><u>Optimal Portfolio Weights</u></p>
            <table class="styled-table">
                <thead>
                    <tr>
                        <th>Ticker</th>
                        <th>Equal-weighted</th>
                        <th>Min Volatility</th>
                        <th>Max Sharpe</th>
                        <th>Risk Parity</th>
                        <th>Momentum</th>
                        <th>Monte Carlo</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>MSFT</td>
                        <td>0.1</td>
                        <td>0</td>
                        <td>0.0897</td>
                        <td>0.1694</td>
                        <td>0.2</td>
                        <td>0.0288</td>
                    </tr>
                    <tr>
                        <td>AAPL</td>
                        <td>0.1</td>
                        <td>0</td>
                        <td>0</td>
                        <td>0.1818</td>
                        <td>0</td>
                        <td>0.0394</td>
                    </tr>
                    <tr>
                        <td>GOOGL</td>
                        <td>0.1</td>
                        <td>0.0557</td>
                        <td>0.0191</td>
                        <td>0.172</td>
                        <td>0.2</td>
                        <td>0.0352</td>
                    </tr>
                    <tr>
                        <td>TSLA</td>
                        <td>0.1</td>
                        <td>0.3308</td>
                        <td>0</td>
                        <td>0</td>
                        <td>0</td>
                        <td>0.04</td>
                    </tr>
                    <tr>
                        <td>JNJ</td>
                        <td>0.1</td>
                        <td>0</td>
                        <td>0</td>
                        <td>0.1687</td>
                        <td>0</td>
                        <td>0.052</td>
                    </tr>
                    <tr>
                        <td>PFE</td>
                        <td>0.1</td>
                        <td>0.0772</td>
                        <td>0</td>
                        <td>0</td>
                        <td>0</td>
                        <td>0.0467</td>
                    </tr>
                    <tr>
                        <td>XOM</td>
                        <td>0.1</td>
                        <td>0.1855</td>
                        <td>0</td>
                        <td>0</td>
                        <td>0.2</td>
                        <td>0.0285</td>
                    </tr>
                    <tr>
                        <td>CVX</td>
                        <td>0.1</td>
                        <td>0.0138</td>
                        <td>0.3034</td>
                        <td>0.1376</td>
                        <td>0.2</td>
                        <td>0.3306</td>
                    </tr>
                    <tr>
                        <td>WMT</td>
                        <td>0.1</td>
                        <td>0.2505</td>
                        <td>0.4813</td>
                        <td>0</td>
                        <td>0.2</td>
                        <td>0.2942</td>
                    </tr>
                    <tr>
                        <td>PG</td>
                        <td>0.1</td>
                        <td>0.0864</td>
                        <td>0.1065</td>
                        <td>0.1705</td>
                        <td>0</td>
                        <td>0.1046</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <p><strong><i><u>Observations on the tables above</u></i></strong>:</p>
        <p><strong><u>A. Performance Analysis</u></strong>:</p>
        <ol>
          <li><strong>Return</strong>: The Maximum Sharpe Ratio (Max Sharpe) and Monte Carlo portfolios exhibit the highest annual returns, followed closely by the Risk Parity and Momentum portfolios. The Minimum Volatility portfolio has the lowest return, which is expected as it prioritizes minimizing risk over maximizing returns. The Equal-weighted portfolio falls in the middle range.</li>

          <p><strong>Volatility</strong>: As expected, the Minimum Volatility portfolio has the lowest volatility. The Max Sharpe and Monte Carlo portfolios have the highest volatility, reflecting their pursuit of higher returns. Risk Parity and Momentum fall in between. The Equal-weighted portfolio also has middle-of-the-road volatility.</p>

          <li><strong>Sharpe Ratio</strong>: The Max Sharpe portfolio has the highest Sharpe Ratio, which is expected since it was designed to maximize this metric. The Monte Carlo portfolio, which also attempts to maximize the Sharpe Ratio, comes in a close second. The Risk Parity, Equal-weighted, and Momentum portfolios follow, and the Minimum Volatility portfolio has the lowest Sharpe Ratio.</li>

          <li><strong>Sortino Ratio</strong>: The Sortino Ratio, which focuses on downside risk, paints a similar picture to the Sharpe Ratio.Max Sharpe (1.6666) and Monte Carlo (1.5733) perform well, indicating strong returns relative to downside risk, on the other hand, Min Volatility (0.6195) again underperforms due to its low returns.</li>

          <li><strong>Omega Ratio</strong>: Monte Carlo (0.005) and Risk Parity (0.0026) have the highest Omega Ratios, suggesting better probability-weighted gains relative to losses, while Equal-weighted (0.0017) and Minimum Volatility (0.0013) portfolios have the lowest Omega Ratios.</li>

          <li><strong>Treynor Ratio</strong>: Which shows excess return per unit of market risk, had Max Sharpe (0.359) and Monte Carlo (0.3408) deliver the highest Treynor Ratios, indicating strong performance relative to market risk, while Minimum Volatility (0.1366) has the lowest Treynor Ratio.</li>

          <li><strong>Information Ratio</strong>: The Momentum (1.3054) and Risk Parity (1.2762) portfolios have the highest Information Ratios, indicating strong outperformance relative to a benchmark. The Minimum Volatility (-0.0878) portfolio underperforms the benchmark.</li>

          <li><strong>Ulcer Index</strong>: The Minimum Volatility (0.0483) portfolio has the lowest Ulcer Index, reflecting its focus on reducing drawdown severity. The Monte Carlo (0.1355) and Momentum (0.1081) portfolios have the highest Ulcer Index, consistent with their higher-risk strategies.</li>

          <li><strong>Sterling Ratio</strong>: The Risk Parity (4.5093) and Max Sharpe (4.4992) portfolios have the highest Sterling Ratios, showing strong returns relative to drawdowns. The Monte Carlo (3.3948) portfolio has the lowest Sterling Ratio, as its high returns come with significant drawdowns.</strong></li>
        </ol>

        <p><strong><u>B. Weight Allocation</u></strong>:</p>
        <ol>
        <li><strong>Equal-weighted</strong>: As the name suggests, this portfolio assigns an equal weight (10%) to each of the 10 assets.</li>

        <li><strong>Minimum Volatility</strong>: This portfolio heavily favors TSLA, followed by WMT, XOM, and PFE. It avoids MSFT, AAPL, and JNJ entirely, likely due to their higher volatility or correlation with other assets.</li>

        <li><strong>Max Sharpe</strong>: This portfolio concentrates its weights in CVX and WMT, with a small allocation to MSFT and PG. Other assets receive no allocation, suggesting these two assets offer the best risk-adjusted returns.</li>

        <li><strong>Risk Parity</strong>: This portfolio has a more balanced allocation, spreading weights across several assets.</p>

        <li><strong>Momentum</strong>: This portfolio invests only in XOM, CVX, WMT, and MSFT, which are the stocks with the highest recent momentum.</li>

        <li><strong>Monte Carlo</strong>: This portfolio also has a relatively concentrated allocation, though it differs from the Max Sharpe portfolio.</li>
        </ol>
        <br />

        <h2><strong>5. Conclusion</strong>:</h2>

        <p>The analysis highlights the classic trade-off between risk and return, with higher-return portfolios like <strong>Max Sharpe</strong> and <strong>Monte Carlo</strong> exhibiting greater volatility and drawdowns. These portfolios deliver the best risk-adjusted performance (Sharpe, Sortino, Treynor Ratios) but are better suited for aggressive investors. On the other hand, the <strong>Minimum Volatility</strong> portfolio is ideal for risk-averse investors, offering the lowest risk and drawdowns, albeit with lower returns. For a balanced approach, the <strong>Risk Parity</strong> portfolio stands out, providing strong risk-adjusted returns and moderate drawdowns. The <strong>Momentum</strong> portfolio, while high-performing, is also high-risk, making it suitable for aggressive investors. The <strong>Equal-weighted</strong> portfolio, though simple and diversified, underperforms in risk-adjusted metrics compared to optimized strategies. Ultimately, there is no one-size-fits-all solution; the "best" portfolio depends on an investor's specific goals, risk tolerance, and time horizon.</p>

        <p><strong><u>Recommendations</u></strong></p>
        <ul>
            <li><strong>For Conservative Investors</strong>: Use Min Volatility to prioritize capital preservation.</li>
            <li><strong>For Balanced Investors</strong>: Use Risk Parity or Max Sharpe for optimal risk-adjusted returns.</li>
            <li><strong>For Aggressive Investors</strong>: Use Momentum or Monte Carlo to maximize returns, accepting higher risk.</li>
            <li><strong>For Passive Investors</strong>: Use Equal-weighted for simplicity and diversification, but be aware of its limitations.</li>
        </ul>

        <p>Each portfolio has trade-offs, and the choice depends on the investor’s risk tolerance, return expectations, and investment horizon. Combining strategies (e.g., using <strong>Risk Parity</strong> for core holdings and <strong>Momentum</strong> for satellite allocations) can also be effective.</p>
        <br />

        <h2><strong>References</strong>:</h2>
        <p>Markowitz, H. (1952). Portfolio selection. The Journal of Finance, 7(1), 77–91. https://doi.org/10.1111/j.1540-6261.1952.tb01525.x</p>
        <p>Sharpe, W. F. (1966). Mutual fund performance. The Journal of Business, 39(1), 119–138. https://doi.org/10.1086/294846</p>
        <p>Sortino, F. A., & Price, L. N. (1994). Performance measurement in a downside risk framework. The Journal of Investing, 3(3), 59–64. https://doi.org/10.3905/joi.1994.408447</p>
        <p>Keating, C., & Shadwick, W. F. (2002). A universal performance measure. The Journal of Performance Measurement, 6(3), 59–84. </p>
        <p>Treynor, J. L. (1965). How to rate management of investment funds. Harvard Business Review, 43(1), 63–75.</p>
        <p>Qian, E. (2005). Risk parity portfolios: Efficient portfolios through true diversification. PanAgora Asset Management.</p>
        <p>Jegadeesh, N., & Titman, S. (1993). Returns to buying winners and selling losers: Implications for stock market efficiency. The Journal of Finance, 48(1), 65–91. https://doi.org/10.1111/j.1540-6261.1993.tb04702.x </p>
        <p>Metropolis, N., & Ulam, S. (1949). The Monte Carlo method. Journal of the American Statistical Association, 44(247), 335–341. https://doi.org/10.1080/01621459.1949.10483310</p>
        <p>Fabozzi, F. J., Kolm, P. N., Pachamanova, D. A., & Focardi, S. M. (2007). Robust portfolio optimization and management. John Wiley & Sons.</p>
        <p>Grinold, R. C., & Kahn, R. N. (2000). Active portfolio management: A quantitative approach for producing superior returns and selecting superior returns and controlling risk (2nd ed.). McGraw-Hill.</p>
        <p>Martin, J. D., & McCann, B. (1989). The investor’s guide to Fidelity funds: Winning strategies for mutual fund investors. John Wiley & Sons.</p>
        <p>Kritzman, M., & Page, S. (2003). The hierarchy of investment choice. Financial Analysts Journal, 59(4), 24–33. https://doi.org/10.2469/faj.v59.n4.2547</p>
        <p>Haugen, R. A., & Baker, N. L. (1991). The efficient market inefficiency of capitalization-weighted stock portfolios. The Journal of Portfolio Management, 17(3), 35–40. https://doi.org/10.3905/jpm.1991.409335</p>
        <p>Michaud, R. O., & Michaud, R. O. (2008). Efficient asset management: A practical guide to stock portfolio optimization and asset allocation (2nd ed.). Oxford University Press.</p>
        <p>Géron, A. (2019). Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow (2nd ed.). O’Reilly Media.</p>

      </div>
      <div class="modal-footer">
        <a href="QuantFolio.zip" download class="download-button">Download Code</a>
      </div>
    </div>
  </div>

  <div id="modal5" class="modal">
    <div class="modal-content">
      <span class="close">&times;</span>
      <h2 class="modal-title">Binary Distances Through Similarity-Dissimilarity Measures</h2>
      <div class="modal-body">
        <br />
        <hr />
        <h3>Abstract</h3>
        <p>This project involves calculating binary distances between rows or columns of a matrix using similarity-dissimilarity measures. It is implemented in R.</p>
        <hr />
        <hr />
        <br>

        <h3>Introduction</h3>
        <p>Binary distance measures are widely used in clustering and classification tasks. This project explores the use of 48 similarity measures.</p>
        <br>

        <h3>Methodology</h3>
        <p>Should we want to compare two objects 𝑈 and 𝑉 as far as their similarity is concerned, then in the binary case there are only four possible outcomes, which are presented in the table below: </p>

        <p><u><i>Table 1: A 2x2 Contingency Table</i></u>:</p>
        <table style="width: 50%; border-collapse: collapse; margin: 20px auto; font-family: sans-serif;">
          <thead>
            <tr>
              <th rowspan="2" style="padding: 10px; border: 1px solid #ddd; text-align: center; vertical-align: middle;">Object V</th>
              <th colspan="3" style="padding: 10px; border: 1px solid #ddd; text-align: center;">Object U</th>
            </tr>
            <tr>
              <th style="padding: 10px; border: 1px solid #ddd; text-align: center;">1</th>
              <th style="padding: 10px; border: 1px solid #ddd; text-align: center;">0</th>
              <th style="padding: 10px; border: 1px solid #ddd; text-align: center;">Total</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">1</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">a</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">b</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">a + b</td>
            </tr>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">0</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">c</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">d</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">c + d</td>
            </tr>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center; font-weight: bold;">Total</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center; font-weight: bold;">a + c</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center; font-weight: bold;">b + d</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center; font-weight: bold;">a + b + c + d</td>
            </tr>
          </tbody>
        </table>

        <p>What we can say about these two objects is how similar or dissimilar they are. If they share the same number of ones and similar to each other, if however they do not, then these two objects are characterized as dissimilar. Here are some similarity measures that use the table above (Seung-Seok Choi, Sung-Hyuk Cha, Charles C. Tappert, 2009):</p>

        <p><u><i>Table 2: Similarity-Dissimilarity Measures</i></u>:</p>
        <table style="width: 100%; border-collapse: collapse; margin: 20px auto; font-family: sans-serif;">
          <thead>
            <tr>
              <th style="padding: 10px; border: 1px solid #ddd; text-align: center;">Similarity Measure</th>
              <th style="padding: 10px; border: 1px solid #ddd; text-align: center;">Formula</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Jaccard or Tanimoto</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{a}{a + b + c}$$</td>
            </tr>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Sorensen-Dice or Czekanowski or Nei & Li</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{2a}{2a + b + c}$$</td>
            </tr>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">3W-Jaccard</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{3a}{3a + b + c}$$</td>
            </tr>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Simple Matching or Sokal & Michener</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{a + d}{a + b + c + d}$$</td>
            </tr>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Sokal & Sneath I</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{a}{a + 2(b + c)}$$</td>
            </tr>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Sokal & Sneath II</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{2(a + d)}{2a + b + c + 2d}$$</td>
            </tr>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Sokal & Sneath III</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{a + d}{b + c}$$</td>
            </tr>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Sokal & Sneath IV</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{1}{4}(\frac{a}{a+b}+\frac{a}{a+c}+\frac{d}{b+d}+\frac{d}{c+d})$$</td>
            </tr>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Sokal & Sneath V or Ochiai II</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{ad}{\sqrt{(a + b)(a + c)(b + d)(c + d)}}$$</td>
            </tr>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Rogers & Tanimoto</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{a + d}{a + 2(b + c) + d}$$</td>
            </tr>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Faith</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{a + 0.5d}{a + b + c + d}$$</td>
            </tr>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Gower & Legendre</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{a + d}{a + 0.5(b + c) + d}$$</td>
            </tr>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Russell & Rao</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{a}{a + b + c + d}$$</td>
            </tr>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Ochiai I or Cosine or Otsuka-Ochiai or Fowlkes-Mallows Index</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{a}{\sqrt{(a + b)(a + c)}}$$</td>
            </tr>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Forbes I</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{na}{(a + b)(a + c)}$$</td>
            </tr>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Forbes II</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{na - (a + b)(a + c)}{n[\min(a + b, a + c) - (a + b)(a + c)]}$$</td>
            </tr>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Fossum</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{n(a - 0.5)^2}{(a + b)(a + c)}$$</td>
            </tr>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Sorgenfrei</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{a^2}{(a + b)(a + c)}$$</td>
            </tr>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Mountford</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{a}{0.5(ab + ac) + bc}$$</td>
            </tr>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">McConnaughey</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{a^2 - bc}{(a + b)(a + c)}$$</td>
            </tr>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Tarwid</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{na - (a + b)(a + c)}{na + (a + b)(a + c)}$$</td>
            </tr>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Kulczynski I</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{a}{b + c}$$</td>
            </tr>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Kulczynski II or Driver & Kroeber</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{1}{2}\left(\frac{a}{a + b} + \frac{a}{a + c}\right)$$</td>
            </tr>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Johnson</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{a}{a + b} + \frac{a}{a + c}$$</td>
            </tr>
            <tr>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Dennis</td>
              <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{ad - bc}{\sqrt{n(a + b)(a + c)}}$$</td>
            </tr>
            <tr>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Simpson or Szymkiewicz-Simpson</td>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{a}{\min(a + b, a + c)}$$</td>
            </tr>
            <tr>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Braun-Blanquet</td>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{a}{\max(a + b, a + c)}$$</td>
            </tr>
            <tr>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Fager & McGowan</td>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{a}{\sqrt{(a + b)(a + c)}} - \frac{\max(a + b, a + c)}{2}$$</td>
            </tr>
            <tr>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Gower</td>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{a + d}{\sqrt{(a + b)(a + c)(b + d)(c + d)}}$$</td>
            </tr>
            <tr>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Pearson I</td>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{n(ad - bc)^2}{(a + b)(a + c)(b + d)(c + d)}$$</td>
            </tr>
            <tr>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Pearson II</td>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\sqrt{\frac{\frac{n(ad - bc)^2}{(a + b)(a + c)(b + d)(c + d)}}{n+\frac{n(ad - bc)^2}{(a + b)(a + c)(b + d)(c + d)}}}$$</td>
            </tr>
            <tr>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Pearson III</td>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\sqrt{\frac{\frac{ad - bc}{\sqrt{(a + b)(a + c)(b + d)(c + d)}}}{n+\frac{ad - bc}{\sqrt{(a + b)(a + c)(b + d)(c + d)}}}}$$</td>
            </tr>
            <tr>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Pearson & Heron I</td>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{ad - bc}{\sqrt{(a + b)(a + c)(b + d)(c + d)}}$$</td>
            </tr>
            <tr>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Pearson & Heron II</td>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\cos{(\frac{\pi\sqrt{bc}}{\sqrt{ad}+\sqrt{bc}})}$$</td>
            </tr>
            <tr>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Cole</td>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{\sqrt{2}(ad - bc)}{\sqrt{(ad - bc)^2 - (a + b)(a + c)(b + d)(c + d)}}$$</td>
            </tr>
            <tr>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Stiles</td>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\log_{10}(\frac{n(|ad - bc| - n/2)^2}{ \sqrt{(a + b)(a + c)(b + d)(c + d)}})$$</td>
            </tr>
            <tr>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Yule’s Q</td>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{ad - bc}{ad + bc}$$</td>
            </tr>
            <tr>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Yule’s ω or Yule’s Y</td>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{\sqrt{ad} - \sqrt{bc}}{\sqrt{ad} + \sqrt{bc}}$$</td>
            </tr>
            <tr>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Dispersion</td>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{ad - bc}{(a + b + c + d)^2}$$</td>
            </tr>
            <tr>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Hamann</td>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{(a + d) - (b + c)}{a + b + c + d}$$</td>
            </tr>
            <tr>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">McEwen & Michael</td>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{4(ad - bc)}{(a + d)^2 + (b + c)^2}$$</td>
            </tr>
            <tr>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Goodman & Kruskal’s Lambda</td>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{\sigma - \sigma'}{2n - \sigma'} \; with: \; \sigma = \max(a, b) + \max(a, c) + \max(b, d) + \max(c, d) \sigma' = \max(a + c, b + d) + \max(a + b, c + d)$$</td>
            </tr>
            <tr>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Anderberg</td>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{\sigma - \sigma'}{2n} \; with: \; \sigma = \max(a, b) + \max(a, c) + \max(b, d) + \max(c, d) \sigma' = \max(a + c, b + d) + \max(a + b, c + d)$$</td>
            </tr>
            <tr>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Baroni-Urbani & Buser I</td>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{\sqrt{ad} + a}{\sqrt{ad} + a + b + c}$$</td>
            </tr>
            <tr>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Baroni-Urbani & Buser II</td>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{\sqrt{ad} + a - (b + c)}{\sqrt{ad} + a + b + c}$$</td>
            </tr>
            <tr>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Peirce</td>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{ad - bc}{(a + b)(c + d)}$$</td>
            </tr>
            <tr>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Eyraud</td>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{a - (a + b)(a + c)}{(a + b)(a + c)(b + d)(c + d)}$$</td>
            </tr>
            <tr>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">Tarantula</td>
                <td style="padding: 10px; border: 1px solid #ddd; text-align: center;">$$\frac{\frac{a}{a + b}}{\frac{a}{a + b} + \frac{c}{c + d}}$$</td>
            </tr>
        </table>

        <p>The distance is calculated in the following manner:</p>
        $$ Distance = 1-Similarity \; Measure $$

        <p>How one picks a suitable distance depends on how they want to treat the zero-zero similarity matches (or 𝑑 in the table above), as well as the dissimilarities between the objects (meaning 𝑏 and 𝑐 in the table above).</p>
        <br>

        <h2>References</h2>
        <p>Choi, Seung-Seok, Sung-Hyuk Cha, and Charles C. Tappert. (2010). <em>A Survey of Binary Similarity and Distance Measures</em>. Journal on Systemics, Cybernetics and Informatics, 8>, 43–48.</p>
        <p>Hubálek, Zdenek. (2008). <em>Coefficients of association and similarity, based on binary (presence-absence) data: an evaluation</em>. Biological Reviews, 57(4), 669–689. https://doi.org/10.1111/j.1469-185X.1982.tb00376.x</p>
        <p>Peirce, Charles S. (1884). <em>The numerical measure of the success of predictions</em>. Science, 4(93), 453–454. https://doi.org/10.1126/science.ns-4.93.453-a</p>
        <p>Jones, James A., & Harrold, Mary Jean. (2005). <em>Empirical evaluation of the tarantula automatic fault localization technique</em>. In *ASE '05 Proceedings of the 20th IEEE/ACM international Conference on Automated software engineering* (pp. 273–282). ACM. https://doi.org/10.1145/1101908.1101949</p>

      </div>
      <div class="modal-footer">
        <a href="binary_distances_code.zip" download class="download-button">Download Code</a>
      </div>
    </div>
  </div>

  <script src="script.js"></script>
</body>
</html>
